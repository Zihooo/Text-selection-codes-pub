{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zihooo/Text-selection-codes-pub/blob/main/The_TFIDF_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZVjSGx2hzd"
      },
      "source": [
        "# The Feature Extraction Approach for Personality Score Prediction\n",
        "This colab is written in **Python** to illistrate the process of *feature extraction approach* with TF-IDF scores and a random forest classifier when predicting personality scores from texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gsl9k5J_zEX"
      },
      "source": [
        "### **Step 1 Text Preprocessing** \n",
        "In the text preprocessing phase, we 1. Removed the special characters. 2. Tokenized the texts. 3. Lowercased all texts. 4. Removed stop words. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive to get access to the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfgWVbpqSmmM",
        "outputId": "7b33f0c1-9b7c-428f-c358-df01262aed4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlXg7JiwR9nN"
      },
      "outputs": [],
      "source": [
        "# import required pacakges\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import raw data\n",
        "csv_file = '/content/drive/MyDrive/Files/Text Selection Paper Codes/data/All_data.csv' # path to data file\n",
        "df = pd.read_csv(csv_file, encoding= 'unicode_escape')\n"
      ],
      "metadata": {
        "id": "aDqn_eTySOiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function to remove special characters and punctuations\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree"
      ],
      "metadata": {
        "id": "EGovw9XbaKxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the and punctuations free text\n",
        "df['clean_text']= df['All_response_raw'].apply(lambda x:remove_punctuation(x))"
      ],
      "metadata": {
        "id": "wbGrQYASbCdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercase the texts\n",
        "df['msg_lower']= df['clean_text'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "-Y-UyeBhYq1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying function to the column\n",
        "df['msg_tokenied']= df['msg_lower'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "bmp2UsvZbmgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-defined stop words dictionary\n",
        "stop_words = stopwords.words('english')\n",
        "# Extend the stop words distionary with some high frequency words in the current data\n",
        "stop_words.extend([\"would\",\"dont\",\"could\",\"id\",\"X\"])"
      ],
      "metadata": {
        "id": "6UDYbRjQWnit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define remove stop words function\n",
        "def remove_english_stopwords_func(text):\n",
        "    # check in lowercase \n",
        "    t = [token for token in text if token.lower() not in stop_words]\n",
        "    text = ' '.join(t)    \n",
        "    return text"
      ],
      "metadata": {
        "id": "Kpw6UJ13c3KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words\n",
        "df['No_Stop_Words'] = df['msg_tokenied'].apply(remove_english_stopwords_func)"
      ],
      "metadata": {
        "id": "CUz1bSpXWT74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2 feature extraction** \n",
        "In the feature extraction phase, we generate the TF-IDF vectors. "
      ],
      "metadata": {
        "id": "D5JOQ3mqV0fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = df.No_Stop_Words"
      ],
      "metadata": {
        "id": "xhTnhBnNir7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate TF-IDF vectors with 2000 features\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "vectors = vectorizer.fit_transform(document)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "tfidf = pd.DataFrame(denselist, columns=feature_names)"
      ],
      "metadata": {
        "id": "saHOZdwWShux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add labels to TF-IDF vectors\n",
        "tfidf['ascore'] = df['ascore']\n",
        "tfidf['cscore'] = df['cscore']\n",
        "tfidf['nscore'] = df['nscore']\n",
        "tfidf['escore'] = df['escore']\n",
        "tfidf['oscore'] = df['oscore']\n",
        "tfidf['split'] = df['split_set']          # we have already split the original data into training and testing set"
      ],
      "metadata": {
        "id": "GURFo0Eu4DzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the TF-IDF scores\n",
        "# tfidf.to_csv('/content/drive/MyDrive/personality prediction/tfidf/tfidf.csv')  # after the file has been saved, it was further splited into a training, evaluation and a testing set."
      ],
      "metadata": {
        "id": "tGduVocxlzCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3 Score Prediction** \n",
        "In the score prediction phase, we used a random forest model to predict personality scores based on TF-IDF vectors. We used the prediction  of Extraversion scores as an example in the current code sample. Other predictions can be achieved by changing the label column."
      ],
      "metadata": {
        "id": "QE7I4LV456f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required pacakges\n",
        "!pip install scipy\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "Biotx2_j8Zsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load previously splited training and testing set\n",
        "train_set = pd.read_csv('/content/drive/MyDrive/Text Selection Paper Codes/data/train_tfidf.csv', encoding= 'unicode_escape')\n",
        "test_set = pd.read_csv('/content/drive/MyDrive/Text Selection Paper Codes/data/test_tfidf.csv', encoding= 'unicode_escape')"
      ],
      "metadata": {
        "id": "T0TD-jJlj-kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifiy the first 2000 columns as features, and the personality scores as labels\n",
        "train_features = train_set.iloc[:,0:2000]\n",
        "test_features = test_set.iloc[:,0:2000]\n",
        "train_labels = train_set.escore\n",
        "test_labels = test_set.escore\n",
        "feature_list = list(train_features.columns)"
      ],
      "metadata": {
        "id": "tCLYGgt1kUKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set parameter grid for grid search\n",
        "param_grid = {'max_depth': [10, 50, 100],\n",
        " 'n_estimators': [200, 600, 1000]}"
      ],
      "metadata": {
        "id": "sEryqTE9J8JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random search of parameters, using 5 fold cross validation, \n",
        "# search across 9 different combinations, and use all available cores\n",
        "# 45 fits in total\n",
        "rf = RandomForestRegressor()\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(train_features, train_labels)"
      ],
      "metadata": {
        "id": "YBIyxoIZKu-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get best estimators\n",
        "best_grid = rf_random.best_estimator_"
      ],
      "metadata": {
        "id": "4NaS8VsNkKcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the model we are using\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Instantiate model with best parameters from random search\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 100)\n",
        "# Train the model on training data\n",
        "rf.fit(train_features, train_labels);"
      ],
      "metadata": {
        "id": "DLiHgtjy61jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(test_features)\n",
        "# get the correlation between predicted scores and labels\n",
        "pearsonr(predictions,test_labels)"
      ],
      "metadata": {
        "id": "Jirx1FLZ7CFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the predicted scores\n",
        "import pandas\n",
        "dfpred = pd.DataFrame(predictions)\n",
        "dfpred.to_csv('/content/drive/MyDrive/personality prediction/final-saved outputs/TFIDF/test_O.csv')"
      ],
      "metadata": {
        "id": "KdA7ehnYlSI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}