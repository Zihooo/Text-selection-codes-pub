{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zihooo/Text-selection-codes-pub/blob/main/Prediction_Model_(RoBERTa_and_Longformer).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZVjSGx2hzd"
      },
      "source": [
        "# Transformer Models for Personality Score Prediction\n",
        "This colab is written in **Python** to illistrate the process of *fine-tuning*  state-of-the-art **Transformer** models to predict personality scores. In this context the fine-tuning process involves training models with a relatively small amount of samples with known trait scores. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIae_k5emARe",
        "outputId": "c2cc7de7-bd02-46c6-ccc7-73158e0b6486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive to get access to the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7S6aRPS_w63",
        "outputId": "08512ae0-ea2c-42fc-debe-a00d896c8c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "## install required pacakges\n",
        "! pip install transformers==4.28.0\n",
        "! pip install sentencepiece\n",
        "! pip install datasets\n",
        "! pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A25eSs8QUkS8"
      },
      "outputs": [],
      "source": [
        "# import pacakges\n",
        "from transformers import AutoConfig, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from warnings import warn\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import pearsonr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0eCZR2ngddV"
      },
      "source": [
        "### Using a GPU\n",
        "To speed things up you can use a *GPU* (*optional*).\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, confirm that you can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXnDmXR7RDr2"
      },
      "outputs": [],
      "source": [
        "# A helper function to check for a GPU\n",
        "def get_gpu ():\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return torch.cuda.current_device()\n",
        "  else:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXevB2XPpocQ",
        "outputId": "49a88580-e5c3-400b-ed4f-9ddbdb5de226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE95PDPffa4-"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkXTg-LmUAkH"
      },
      "source": [
        "### Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p21aZOBc9Pvh"
      },
      "outputs": [],
      "source": [
        "#@title Load user-defined utility functions\n",
        "# Import Data function\n",
        "def import_data(path, text_col, label_col, index_col = None, index_val = None, enc = 'latin1'):\n",
        "  \"\"\"Import a CSV of sentences\n",
        "  \n",
        "  Args:\n",
        "    path: A csv file path\n",
        "    text_col: Name of column in csv containing sentences\n",
        "    label_col: Name of column containing labels\n",
        "    enc: File encoding to be used (optional)\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(path, encoding = enc,keep_default_na=False)\n",
        "  if not isinstance(index_val, type(None)):\n",
        "    df = df[df[index_col] == index_val]\n",
        "  if label_col is None:\n",
        "    return df[text_col].tolist(), df\n",
        "  return df[text_col].tolist(), df[label_col].tolist(), df\n",
        "\n",
        "# Map labels to keys\n",
        "#def map_labels_to_keys(labels, sort_labels = True):\n",
        "  \"\"\"Map text labels to integers\n",
        "  \n",
        "  Args:\n",
        "    labels: a list/vector of text labels\n",
        "    sort_labels: Sort labels alphabetically before recoding (optional)\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "# Get model for simple transformers\n",
        "def get_model(model_type):\n",
        "    if  model_type == \"specter\":\n",
        "        model_name = \"allenai/specter\"\n",
        "    elif model_type == \"bert\":\n",
        "        model_name = \"bert-base-cased\"\n",
        "    elif model_type == \"roberta\":\n",
        "        model_name = \"roberta-large\"\n",
        "    elif model_type == \"distilbert\":\n",
        "        model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "    elif model_type == \"distilroberta\":\n",
        "        model_type = \"roberta\"\n",
        "        model_name = \"cross-encoder/stsb-distilroberta-base\"\n",
        "    elif model_type == \"electra-base\":\n",
        "        model_type = \"electra\"\n",
        "        model_name = \"cross-encoder/ms-marco-electra-base\"\n",
        "    elif model_type == \"xlnet\":\n",
        "        model_name = \"xlnet-large-cased\"\n",
        "    elif model_type == \"bart\":\n",
        "        model_name = \"facebook/bart-large\"\n",
        "    elif model_type == \"deberta\":\n",
        "        model_type = \"debertav2\"\n",
        "        model_name = \"microsoft/deberta-v3-large\"\n",
        "    elif model_type == \"albert\":\n",
        "        model_name = \"albert-xlarge-v2\"\n",
        "    elif model_type == \"xlmroberta\":\n",
        "        model_name = \"xlm-roberta-large\"\n",
        "    else:\n",
        "        warnings.warn(\"model_type not a pre-defined, setting model_type to model_name\")\n",
        "        model_name = model_type\n",
        "    return model_type, model_name\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnZLUABzOfmc"
      },
      "outputs": [],
      "source": [
        "# eval metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_selection import r_regression\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def compute_metrics_for_regression(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    mse = mean_squared_error(labels, logits)\n",
        "    r = pearsonr(labels.reshape(-1), logits.reshape(-1))\n",
        "    rscore = r[0].tolist()\n",
        "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
        "    return {\"mse\": mse, \"r\": rscore}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M85LHN2IhlcH"
      },
      "outputs": [],
      "source": [
        "#@title Data Class\n",
        "class TextClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdULdNEfUYb1"
      },
      "source": [
        "### Defining Variables\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We define our variables for purposes described in our research manuscripte. However, we encourage researchers and practitioners to try out alternative models. In addition, we wanted to minimize the tuning hyper-parameters during training as the aim of this research is to highlight Transformers in a baseline sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8tlBiCW5mBy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BASE_MODEL = 'roberta-base' # replace with \"longformer-base-4096\" for longformer\n",
        "LEARNING_RATE = 5e-7\n",
        "MAX_LENGTH = 512      # can be increased to 4096 when use longformer, a longer sequence leads to heavier computation load\n",
        "BATCH_SIZE = 12       # batch size is defined based on available computational resource (GPU memory)\n",
        "EPOCHS = 50           # may increase this number if there is no diminishing reture on evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5mojBFURyy"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Fine-tuning A Transformer Model\n",
        "\n",
        "\n",
        "---\n",
        "This example demonstrates the fine-tuning process for the pupose of score prediction from text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOpPwcYvTvc"
      },
      "source": [
        "### Importing and formatting Training Data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Since we have already mount this notebood at our drive, we can directly import data from Google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD0E9EpLr3-V"
      },
      "outputs": [],
      "source": [
        "#@title Importing custom datasets\n",
        "\n",
        "# \"texta\" refers to the column that contains textual data, \"ascore\" refers to the column that contains labels\n",
        "# the import_data function will return a list of sentences and the original dataset\n",
        "# training set\n",
        "train_text, train_labels, train_raw_data = import_data(\"/content/drive/MyDrive/Text Selection Paper Codes/data/train_relevant_10.csv\", \"textn\", \"nscore\")\n",
        "\n",
        "# evaluation set\n",
        "eval_text, eval_labels, eval_raw_data = import_data(\"/content/drive/MyDrive/Text Selection Paper Codes/data/eval_relevant_10.csv\", \"textn\", \"nscore\")\n",
        "\n",
        "## testing set\n",
        "test_text, test_labels, test_raw_data = import_data(\"/content/drive/MyDrive/Text Selection Paper Codes/data/test_relevant_10.csv\", \"textn\", \"nscore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob7mKIM_8Dpz"
      },
      "source": [
        "To properly import the training data we must specify the file path, column name containing our items, and column name containing our labels. Then, the `import_data()` returns three objects:\n",
        "\n",
        "- a list (vector) of items\n",
        "- a list (vector) of labels\n",
        "- a copy of our training data\n",
        "\n",
        "The code above assigns these to objects names `train_text`, `train_labels` and `raw_data` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXHUQ5SJ6mtO"
      },
      "outputs": [],
      "source": [
        "#@title Tokenize data\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "#train_labels_indx, lab_to_id, num_labs = map_labels_to_keys(train_labels)\n",
        "train_encodings = tokenizer(train_text, truncation=True, max_length = MAX_LENGTH,padding='max_length')\n",
        "train_dataset = TextClassificationDataset(train_encodings, train_labels)\n",
        "    \n",
        "#eval_labels_indx, _, _ = map_labels_to_keys(eval_labels)\n",
        "eval_encodings = tokenizer(eval_text, truncation=True, max_length = MAX_LENGTH,padding='max_length')\n",
        "eval_dataset = TextClassificationDataset(eval_encodings, eval_labels)\n",
        "\n",
        "#test_labels_indx, _, _ = map_labels_to_keys(test_labels)\n",
        "test_encodings = tokenizer(test_text, truncation=True, max_length = MAX_LENGTH,padding='max_length')\n",
        "test_dataset = TextClassificationDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6IMLaY-lu-"
      },
      "source": [
        "### Training the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_tmG_6v-3oC",
        "outputId": "5be98ab9-822f-43e3-aedf-772f15c7c875"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load model\n",
        "MODEL = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1) # problem_type is set to 'regression' when num_labels = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqZw6X-EPbmG"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Text Selection Paper Codes/checkpoints/relevant-n\", # directory to save the model\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    seed = 100,                                                    # thought the seed number for training is fixed here, there is still some randomness in model innitiations. \n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps = 50,\n",
        "    metric_for_best_model=\"mse\", greater_is_better = False,    # This metric can also be r, and change greater is better to True.\n",
        "                                                               # No matter which to use, an observation on the training log is necessary for model selection (avoid automatically selecting the first few epoches).\n",
        "    load_best_model_at_end=True,     # this will save the epoch with the lowest loss metric as final output.\n",
        "    weight_decay=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhkKAWLVBHum"
      },
      "outputs": [],
      "source": [
        "  # initialize trainer\n",
        "trainer = Trainer(model=MODEL,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    compute_metrics = compute_metrics_for_regression,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UnOau0JWBoBY",
        "outputId": "a3eedc0c-6114-4112-b03b-24a663daf1b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2050' max='2050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2050/2050 58:44, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mse</th>\n",
              "      <th>R</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.562261</td>\n",
              "      <td>7.562260</td>\n",
              "      <td>0.059764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>6.999300</td>\n",
              "      <td>6.850716</td>\n",
              "      <td>6.850716</td>\n",
              "      <td>0.091776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.580600</td>\n",
              "      <td>5.580133</td>\n",
              "      <td>5.580133</td>\n",
              "      <td>0.097186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.580600</td>\n",
              "      <td>3.128383</td>\n",
              "      <td>3.128383</td>\n",
              "      <td>0.067326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.696600</td>\n",
              "      <td>1.524704</td>\n",
              "      <td>1.524703</td>\n",
              "      <td>-0.071475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.569800</td>\n",
              "      <td>1.136310</td>\n",
              "      <td>1.136310</td>\n",
              "      <td>-0.076346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.569800</td>\n",
              "      <td>1.118980</td>\n",
              "      <td>1.118980</td>\n",
              "      <td>0.006629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.038900</td>\n",
              "      <td>1.114224</td>\n",
              "      <td>1.114224</td>\n",
              "      <td>0.043580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.954900</td>\n",
              "      <td>1.116863</td>\n",
              "      <td>1.116863</td>\n",
              "      <td>0.089022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.954900</td>\n",
              "      <td>1.111495</td>\n",
              "      <td>1.111495</td>\n",
              "      <td>0.139980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.988400</td>\n",
              "      <td>1.091449</td>\n",
              "      <td>1.091449</td>\n",
              "      <td>0.212248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.928700</td>\n",
              "      <td>1.081491</td>\n",
              "      <td>1.081491</td>\n",
              "      <td>0.248558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.928700</td>\n",
              "      <td>1.073525</td>\n",
              "      <td>1.073525</td>\n",
              "      <td>0.284741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.943300</td>\n",
              "      <td>1.072094</td>\n",
              "      <td>1.072094</td>\n",
              "      <td>0.295964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.963000</td>\n",
              "      <td>1.070492</td>\n",
              "      <td>1.070492</td>\n",
              "      <td>0.373010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.963000</td>\n",
              "      <td>1.054926</td>\n",
              "      <td>1.054926</td>\n",
              "      <td>0.448883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.907600</td>\n",
              "      <td>1.049186</td>\n",
              "      <td>1.049186</td>\n",
              "      <td>0.473574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.930200</td>\n",
              "      <td>1.040147</td>\n",
              "      <td>1.040147</td>\n",
              "      <td>0.480586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.930200</td>\n",
              "      <td>1.033789</td>\n",
              "      <td>1.033789</td>\n",
              "      <td>0.479979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>1.021721</td>\n",
              "      <td>1.021721</td>\n",
              "      <td>0.480778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.903200</td>\n",
              "      <td>1.011318</td>\n",
              "      <td>1.011318</td>\n",
              "      <td>0.482576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.881700</td>\n",
              "      <td>1.001943</td>\n",
              "      <td>1.001943</td>\n",
              "      <td>0.475629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.881700</td>\n",
              "      <td>1.002239</td>\n",
              "      <td>1.002239</td>\n",
              "      <td>0.463361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.876800</td>\n",
              "      <td>0.991268</td>\n",
              "      <td>0.991268</td>\n",
              "      <td>0.479606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.870900</td>\n",
              "      <td>0.982998</td>\n",
              "      <td>0.982998</td>\n",
              "      <td>0.481306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.870900</td>\n",
              "      <td>0.972704</td>\n",
              "      <td>0.972704</td>\n",
              "      <td>0.484680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.811900</td>\n",
              "      <td>0.967235</td>\n",
              "      <td>0.967235</td>\n",
              "      <td>0.483909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.876500</td>\n",
              "      <td>0.955945</td>\n",
              "      <td>0.955945</td>\n",
              "      <td>0.480175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.876500</td>\n",
              "      <td>0.948936</td>\n",
              "      <td>0.948936</td>\n",
              "      <td>0.488173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.834800</td>\n",
              "      <td>0.951232</td>\n",
              "      <td>0.951232</td>\n",
              "      <td>0.483093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.850300</td>\n",
              "      <td>0.931005</td>\n",
              "      <td>0.931005</td>\n",
              "      <td>0.485803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.850300</td>\n",
              "      <td>0.925456</td>\n",
              "      <td>0.925456</td>\n",
              "      <td>0.492281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.817000</td>\n",
              "      <td>0.913882</td>\n",
              "      <td>0.913882</td>\n",
              "      <td>0.495482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.820300</td>\n",
              "      <td>0.909590</td>\n",
              "      <td>0.909590</td>\n",
              "      <td>0.498861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.820300</td>\n",
              "      <td>0.904065</td>\n",
              "      <td>0.904065</td>\n",
              "      <td>0.500042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.829200</td>\n",
              "      <td>0.904015</td>\n",
              "      <td>0.904015</td>\n",
              "      <td>0.502595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.824500</td>\n",
              "      <td>0.898434</td>\n",
              "      <td>0.898434</td>\n",
              "      <td>0.501353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.824500</td>\n",
              "      <td>0.895140</td>\n",
              "      <td>0.895140</td>\n",
              "      <td>0.501509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.749300</td>\n",
              "      <td>0.887736</td>\n",
              "      <td>0.887736</td>\n",
              "      <td>0.501858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.792800</td>\n",
              "      <td>0.887051</td>\n",
              "      <td>0.887051</td>\n",
              "      <td>0.502093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.786700</td>\n",
              "      <td>0.883031</td>\n",
              "      <td>0.883031</td>\n",
              "      <td>0.503381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.786700</td>\n",
              "      <td>0.877131</td>\n",
              "      <td>0.877131</td>\n",
              "      <td>0.503872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.780600</td>\n",
              "      <td>0.872436</td>\n",
              "      <td>0.872436</td>\n",
              "      <td>0.504567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.751900</td>\n",
              "      <td>0.869502</td>\n",
              "      <td>0.869502</td>\n",
              "      <td>0.505559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.751900</td>\n",
              "      <td>0.874138</td>\n",
              "      <td>0.874138</td>\n",
              "      <td>0.506205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.806900</td>\n",
              "      <td>0.872400</td>\n",
              "      <td>0.872400</td>\n",
              "      <td>0.506811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.768800</td>\n",
              "      <td>0.872203</td>\n",
              "      <td>0.872203</td>\n",
              "      <td>0.506022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.768800</td>\n",
              "      <td>0.871205</td>\n",
              "      <td>0.871205</td>\n",
              "      <td>0.506524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.774700</td>\n",
              "      <td>0.870647</td>\n",
              "      <td>0.870647</td>\n",
              "      <td>0.506272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.758800</td>\n",
              "      <td>0.871119</td>\n",
              "      <td>0.871119</td>\n",
              "      <td>0.506272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2050, training_loss=1.2784752375905106, metrics={'train_runtime': 3526.1023, 'train_samples_per_second': 6.863, 'train_steps_per_second': 0.581, 'total_flos': 6367230370406400.0, 'train_loss': 1.2784752375905106, 'epoch': 50.0})"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RUN\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtRKgWMtGIP"
      },
      "source": [
        "### Predict scores with the fine-tuned model\n",
        "\n",
        "---\n",
        "\n",
        "Since we've fined tuned the model we can use the `.predict()` method to predict the target labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WylbWbbER_SQ"
      },
      "outputs": [],
      "source": [
        "# use the same initial model for training\n",
        "trainer.train(resume_from_checkpoint = \"/content/drive/MyDrive/Text Selection Paper Codes/checkpoints/relevant-n/checkpoint-41\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "GwBjB4xyAZwQ",
        "outputId": "c88b52b2-d453-4131-990c-4ace9c8f81c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.8695018887519836,\n",
              " 'eval_mse': 0.8695018887519836,\n",
              " 'eval_r': 0.5055594380513734,\n",
              " 'eval_runtime': 3.8433,\n",
              " 'eval_samples_per_second': 31.744,\n",
              " 'eval_steps_per_second': 2.862,\n",
              " 'epoch': 50.0}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check which epoch was selected\n",
        "trainer.eval_dataset=eval_dataset\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "axFI8aHvF5a9",
        "outputId": "db357ce7-208a-40e7-87be-cc2a489f87f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# run prediction\n",
        "pred_set = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uM19OhbVCOq"
      },
      "outputs": [],
      "source": [
        "# save the predicted results into a list\n",
        "xss = pred_set[0]\n",
        "flat_list = [x for xs in xss for x in xs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx8YZqlkUaKx"
      },
      "outputs": [],
      "source": [
        "# calculate the correlation between predicted scores and labels\n",
        "pearsonr(flat_list,pred_set[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c60FZFzJZZ5r"
      },
      "outputs": [],
      "source": [
        "# save the predicted scores\n",
        "import pd from pandas\n",
        "dfpred = pd.DataFrame(flat_list)\n",
        "#dfpred.to_csv('/content/drive/MyDrive/personality prediction/final-saved outputs/wd/relevance/test_O_epoch8.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}